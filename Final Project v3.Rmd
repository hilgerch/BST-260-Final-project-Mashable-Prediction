---
title: "Final Project v3 hilger"
output: html_document
---


```{r}
library(rvest)
library(tidyverse)
library(dplyr)
```



```{r}

#read in csv

MyData <- read.csv(file="OnlineNewsPopularity.csv", header=TRUE, sep=",", stringsAsFactors = FALSE)

MyDataCopy <- MyData

#MyData_abb <- MyData

#MyData_abb <- MyData[1001:5000,]

#MyData_abb <- head(MyData,5) 

```

```{r}

#get get the urls and make the a list

urls <- data.frame( urls = MyData_abb$url)



```



```{r}
#define the function to get the article content


testFunction <- function (y) {
  return(tryCatch(get_text(y), error=function(e) return("N/A")))
}


get_text <- function(x){
  
  

  
  y <- read_html(x)
  
#get the article content
  
  
  text <- y %>%
  html_node(".article-content") %>%
  html_text()
  
  
#get the author name
   a <- y %>%
    html_node(".author_name a") %>%
  html_text()
  

  if(!is.na(a)) {
    
    a <- a
    
    }
  
  if(is.na(a)) {
    
     b <- y  %>%
       html_node(".basic") %>%
       html_text()
     
     a <- b
  }
   
   
#get the title
  title <- y %>%
  html_node(".title") %>%
  html_text()
  
#get the date
 
  date <- y %>%
  html_node("time") %>%
  html_text()
  
  

  
  return(data.frame(date = date, title = title, author = a, text = text, stringsAsFactors=F))


}





```

```{r}
#title_all <- apply(urls, 1, get_title)
#date_all <- apply(urls, 1, get_date)
#author_all <- apply(urls, 1, get_author)


#this one below works
#text_all <- apply(urls, 1, get_text)

text_all <- do.call("rbind", apply(urls, 1, testFunction))


```



```{r}
#add an id to the left of the data frame

id_df <- data.frame(id = seq.int(nrow(text_all)))

total_df <- cbind(id_df, urls, text_all)



```

```{r}
write.csv(total_df, file = "ScrapedData_10.csv")
```

```{r}
ScrapedDataLoadTest <- read.csv(file="ScrapedData.csv", header=TRUE, sep=",", stringsAsFactors = FALSE)
```

```{r}

ScrapedDataLoadTest1 <- cbind(ScrapedDataLoadTest, shares= MyData$shares)


ScrapedDataLoadTest2 <- ScrapedDataLoadTest1 %>%
  group_by(author) %>%
  mutate(author_count = n())

```



```{r}

Groupby_Author <- ScrapedDataLoadTest2 %>%
  filter(!is.na(author)) %>%
  filter(!is.null(author)) %>%
  filter(author_count > 100) %>%
  group_by(author) %>%
  summarise (median_shares = median(shares))




# 
Groupby_Author %>%
  ggplot(aes(x = author, y = median_shares)) +
  geom_bar(stat = "identity") +
  theme(axis.text.x = element_text(angle = 90, hjust = 1))


#ggplot(aes(x = species_id, y = mean_weight)) + geom_bar(stat = "identity")

# 
# ggplot(diamonds, aes(carat)) +
#   geom_histogram()
  



# g <- ggplot(ScrapedDataLoadTest, aes(author))
# # Number of cars in each class:
# g + geom_bar() 

```


```{r}




Specific_Author <- ScrapedDataLoadTest2 %>%
  filter(author == "Bob Al-Greene") %>%
  filter(!is.na(shares))
```


```{r}
 # ggplot(Charlie, aes(shares)) +
 #  geom_histogram(binwidth = 200000)
 #  


p <- ggplot(Specific_Author, aes(id, shares))
p + geom_boxplot()




```




```{r}
ScrapedDataFull <- read.csv(file="ScrapedDataFull.csv", header=TRUE, sep=",", stringsAsFactors = FALSE)
```

```{r}
ScrapedDataFull[2,]$id <- total_df[2,]$id
ScrapedDataFull[2,]$date <- total_df[2,]$date
ScrapedDataFull[2,]$urls <- total_df[2,]$urls
ScrapedDataFull[2,]$title <- total_df[2,]$title
ScrapedDataFull[2,]$text <- total_df[2,]$text
ScrapedDataFull[2,]$author <- total_df[2,]$author

```


```{r}
write.csv(ScrapedDataFull, file = "ScrapedDataFull.csv")
```


-----------------------

#get rid of useless columns

```{r}
MyData <- MyData %>% select (-c(timedelta, average_token_length, num_keywords, kw_min_min, kw_max_min, kw_avg_min,  kw_min_max,  kw_max_max,  kw_avg_max,kw_min_avg,  kw_max_avg,  kw_avg_avg, LDA_00,  LDA_01, LDA_02,  LDA_03, LDA_04))
```

```{r}

```


# Convert to proper data types

```{r}
#data$Var1 <- as.factor(data$Var1) 

int_indx <- c(2, 3	,4,	5	,6,	7	,8,	9,	10,17,18,44)
MyData[int_indx] <- lapply(MyData[int_indx], function(x) as.integer(x))

factor_indx <- c(11,	12,	13	,14	,15,	16, 20, 	21	,22	,23	,24	,25	,26,	27)
MyData[factor_indx] <- lapply(MyData[factor_indx], function(x) as.factor(x))

num_indx <- c(19,28	,29,	30	,31	,32	,33	,34	,35	,36	,37	,38,	39,	40	,41	,42,	43 )
MyData[num_indx] <- lapply(MyData[num_indx], function(x) as.numeric(x))

#names(data) <- c("new_name", "another_new_name")
```


```{r}

get_table <- function(x) {
  
  x <- as.numeric(x)
  x <- round(x, digits = 2) 
  d <- table(x)
  d <- data.frame(d)
  a <- d$Freq > 10000
  return(sum(a, na.rm=TRUE))
}


```

```{r}


#text_all <- apply(urls, 1, get_text)
#text_all <- do.call("rbind", apply(urls, 1, testFunction))


#d %>% filter(count > 200) %>% nrow()

d <- apply(MyData[,c(2:44)], 2, get_table)

#d <- data.frame(d)
#df[,c(1,2,5)]


#table(round(MyData$global_rate_negative_words,digits=2))

```


```{r}
# attribute_df <- data.frame(table(round(CombinedData$shares,digits=2)))
# attribute_df_freq <- attribute_df$Freq
# 
# attribute_df$Var1 <- as.factor(attribute_df$Var1)
# 
# 
# 
# ggplot(attribute_df, aes(Var1, Freq)) +
#   geom_histogram()




# geom_bar(stat="identity")
#use par(mfrow)



# g <- ggplot(mpg, aes(class))
# # Number of cars in each class:
# g + geom_bar()
```

```{r}

#doing the data analysis

MyData_abb 

CombinedData <- ScrapedDataFull %>% 
  left_join(MyData_abb,  by = c("urls" = "url"))

```


```{r}
# CombinedData %>% 
#   group_by(author) %>%
  
  
p1 <- CombinedData %>% 
  count(author) %>% 
  ggplot(aes(n)) + 
  geom_histogram() + 
  scale_x_log10() + 
  ggtitle("Users")

p1
```


```{r}
lolz <-CombinedData %>% 
  group_by(author) %>%
  summarize(average_shares = mean(shares), number_of_articles = n())




```

```{r}
set.seed(123)
n_test <- round(nrow(CombinedData) / 10)
test_indices <- sample(1:nrow(CombinedData), n_test, replace=FALSE)
test <- CombinedData[test_indices,]
train <- CombinedData[-test_indices,]

```

```{r}
RMSE <- function(true_ratings, predicted_ratings){
    sqrt(mean((true_ratings - predicted_ratings)^2))
}

```



```{r}
mu <- median(train$shares)
```


```{r}
predictions <- rep(mu, nrow(test))
naive_rmse <- RMSE(test$shares, predictions)

naive_rmse
```

```{r}
rmse_results <- data_frame(method = "Just the average", RMSE = naive_rmse)
```

```{r}
train %>% group_by(author) %>% 
  filter(n()>=10) %>% 
  summarize(shares = mean(shares)) %>% 
  qplot(shares, geom = "histogram", data = .)
```


```{r}
mu <- mean(train$shares) ##we already computed this above
author_means <- train %>% 
  group_by(author) %>% 
  summarize(b_i = mean(shares - mu))

author_means %>% qplot(b_i, geom ="histogram", data = .)
```

```{r}
joined <- test %>% 
  left_join(author_means, by='author')
any( is.na(joined$b_i))



```
```{r}
joined <- replace_na(joined, list(b_i=0))
```


```{r}
predicted_shares <- mu + joined$b_i
model1_rmse <- RMSE(predicted_shares, test$shares)
rmse_results <- bind_rows(rmse_results,
                          data_frame(method="Author Effect Model",  
                                     RMSE = model1_rmse ))
rmse_results %>% kable
```

```{r}
test %>% mutate(prediction = predicted_shares, 
                residual = predicted_shares- test$shares) %>%
  arrange(desc(abs(residual))) %>% 
  left_join(author_means) %>%  
  select(urls, prediction, residual) %>% slice(1:10) %>% kable


#now the worst

author_means <-  left_join(author_means, CombinedData,  by='author') 
```

```{r}
train %>% count(author) %>% left_join(author_means) %>%
  filter(n >10) %>%
  arrange(b_i) %>% select(title, urls, b_i, n) %>% slice(1:30) %>% kable

train %>% count(author) %>% left_join(author_means) %>%
 filter(n >10) %>%
  arrange(desc(b_i)) %>% select(title, urls, b_i, n) %>% slice(1:30) %>% kable
```

